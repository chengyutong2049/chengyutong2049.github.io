---
layout: default
title: Adversarial Demonstration Attacks on Large Language Models
parent: LLM Attack
permalink: /docs/llm-attack/adversial-demo-attack
nav_order: 3
has_children: false
math: mathjax
---

# Adversarial Demonstration Attacks on Large Language Models
{: .no_toc }
{: .fs-9 }
<!-- {: .no_toc } -->

Chaowei Xiao (Wisc)
{: .fs-6 .fw-300 }

- attackers can manipulate only the demonstrations without changing the input to perform an attack.
- the security concern of ICL from an adversarial perspective, focusing on the impact of demonstrations.
- a novel attack method named advICL, which aims to manipulate only the demonstration without changing the input to mislead the models.
- as the number of demonstrations increases, the robustness of in-context learning would decrease.
- Additionally, we also identify the intrinsic property of the demonstrations is that they can be used (prepended) with different inputs.
- a more practical threat model in which an attacker can attack the test input example even without knowing and manipulating it.
- Our experiment shows that the adversarial demonstration generated by TransferableadvICL can successfully attack the unseen test input examples.
- We hope that our study reveals the critical security risks associated with ICL and underscores the need for extensive research on the robustness of ICL, particularly given its increasing significance in the advancement of LLMs.